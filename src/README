
May 29, 2003:    connect1.f --- changed outfldrp and ro:  nr--> nx1, nx2.

March 27, 2003:  For 3D pipe flow with LX1=5, it is important to use LX3=LX1-2
                 and to therefore modify reset_geom() in ic.f to map from mesh 1
                 to mesh 3.   This subparametric formulation should be the nek5000
                 standard.


Dec 10, 2002:   we should modify hmholtz to handle d velocity components (and T ?)
Dec 10, 2002:   navier4.f - delete ssnormp call, which does nothing
Dec 10, 2002:   try to clean up unnecessary AXHELM and glsum calls, by
                eliminating needless norm checks (e.g,. chktcg1)

Dec  5, 2002:   navier1.f - only 2 gops/cg iteration

Dec  2, 2002:   implement Chebyshev iteration

Sep 28, 2002:   Fixed map2.f.  Reassigned ip,is,it so that gfdm works in 2D.

Aug 12, 2002:   Updated navier4_div.f to allow nonzero divergence to fix outflow turbulent bc 

Aug 12, 2002:   Updated ZPER for more memory, and gfdm_solve.f to handle 2D.

Aug 12, 2002:   Fixed prepost.f to not dump his info for 'avg', 'rms', etc.

Aug 11, 2002:   Fixed ic.f to support up to 4 passive scalars in ascii restart

July 7, 2002:   Fixed prepost.f to support more than 9 fields in output.

June ?, 2002,   .... gfdm_op.f nn, nel

May 22, 2002 Updates (also, listing differences from hmt code)

coef.f
  . upgraded chkjac() refs to issue collective "call exitt" upon failure

navier1.f:
  . new dealiasing algorithm
  . 2nd anelastic formulation

navier4.f:
  . more robust treatment for loss of orthogonality in E-conj vectors

navier5.f:
  . added "scale" to drag-calc
  . changed filter diagnostics for node 0 only


May 16, 2002 pff

todo:

  navier1.f - ensure that anelastic2 formulation is in place
            - ensure that dealias p99=3 option is set

  fast3d.f  - ensure that plane_space2 is in place

  get nc5.f set up

  get new convective operator set

  if flag.eq.false in comm_mpi

  fix libtfs to allow 48 processors



April    2,2002 pff

o  Update navier4.f to handle loss of orthogonality in pressure projection
   spaces.  This is sometimes necessary if the boundary conditions change.


March   25,2002 pff

o  lbcast logic in ic.f fixed

January 20,2002 pff

o  drive.f now supports constant volume flow rate in x, y, or z
   direction, depending on whether param54 is 1,2,or 3.  x=default
   for any other value of p54.

o  Global fdm method incorporated as solver and preconditioner for
   E solve when p116=nelx, p117=nely, p118=nelz.   Note that setting
   one of these to its negative value determins the "primary" direction
   that is wholly contained within each processor.  Thus, if, for example,
   nelx = -6, nely = 8 and nelz = 5, then the 8x5 array of elements would
   be partitioned among P processors, and each processor would receive
   (8x5)/P stacks of depth 6.   It is thus relatively important that
   the product of the number of elements in the remaining secondary
   and tertiary directions (those not flagged by a minus sign) should
   be a multiple of P.

o  Dealiasing is currently enabled, whenever p99=2.   There is some memory
   savings to be had if dealiasing is not being used by editing DEALIAS
   and commenting out the appropriate parameter statements.

o  The comm_mpi routine has been cleaned up.   All vector reductions are
   performed using mpi_all_reduce so, in theory, there should not be a 
   constraint that P=2**cd, provided one isn't using the XXt solver (e.g.,
   if one is using the gfdm solver).

o  The XXt solver is almost in place for the steady conduction case.

--------------------------------------------------------------------------------


9/10/01, pff

The routines navier5.f and connect1.f have been modified to allow for multiple
passive scalars.



--------------------------------------------------------------------------------

Using Nekton 2ex.

This is a research variant of the commercial code, Nekton 2.0, developed in
the late 80's by P. Fischer, L. Ho, and E. Ronquist, with technical input from
A. Patera and Y. Maday.   The graphics were developed by P. Fischer and 
E. Bullister.

Modifications subseqent to 1991 were made by P. Fischer and H. Tufo.



Nekton consists of three principal modules:  the pre-processor, the solver, 
and the post-processor.  The pre- and post-processors, (resp. prenek and 
postnek) are based upon an X-windows GUI.   Recently, postnek has been 
extended to output Visual Toolkit (vtk) files, so that output my be viewed 
in the cave.  This is still under development.   

The solver, nekton, is written in F77 and C and supports either MPI or 
Intel's NX message passing libraries.   It is a time-stepping based code 
and does not currently support steady-state solvers, other than steady 
Stokes and steady heat conduction.


-----------------------------------------------------------------------------

To get started, you create a directory, say, nekton, and unpack the tarfile:

mkdir  nekton
mv tarfile.gz nekton
cd nekton
gunzip tarfile
tar -xvf tarfile
rm tarfile

The tarfile will put the source code in the directory src, and will
create additional subdirectories: nekton/bin, nekton/2d8, nekton/3d6,
and nekton/prep.

nekton/bin contains the shell scripts which I use to start a job.
I usually keep these files in my /bin directory at the top level.   
The scripts will be of use so you can see how I manage the files 
created by nekton.

nekton/3d6 (2d8) contains an example .rea file, with associated .fld00 file
for restarting.  Once you "gunzip *" in all of the directories
you should be able to build the executable by typing "makenek" in 
the 3d6 directory.  

For each job that you run, you need a corresponding "subuser.f" file in the
nekton/src directory.  "subuser.f" provides the user supplied function 
definitions (e.g., for boundary conditions, initial conditions, etc).  I 
usually keep several subuser.f files in the nekton directory for different 
cases under the name of *.user, e.g., channel.user for channel flow, or
cyl.user for flow past a cylinder, etc.  


To build the source, simply

cd 3d6
makenek   (or,  "makebk" to make in the background...)

To run it, type

nekb co1a

This will run the job entitled "co1a" in background.  The scripts I have
set up in nekton/bin are:

   nek:    runs interactively, 

   nekl:   runs interactively, but redirects std. out to job.log ( & logfile)

   nekb:   like nekl, but in the background

   nekd:   like nek, but with dbx

Each looks for the session name (the prefix of the "*.rea" file) as
an argument.

To  build the pre- and post-processors, you will  cd to nekton/prep.
Type "make" to make postx, and "make -f mpre" to build prex.

If you then put postx and prex into your top-level bin directory
you'll be able to invoke the pre and post-processors with the 
commands "prex" or "postx", etc.  (I assume you're already somewhat
familiar w/ using pre and post?).

OK... I'm going to stop here.  If I can, I'll try to make a more
comprehensive user manual.  In the meantime I can help boot you up
via email  (pff@cfm.brown.edu).

Best,

Paul  

-------------------------------------------------------------


Here's a brief explanation of the nekton input format.

First, an overview of the structure of the file:

Section I:   Parameters, logical switches, etc.

  This section tells nekton whether the input file reflects
  a 2D/3D job, what combination of heat transfer/ Stokes /
  Navier-Stokes/ steady-unsteady / etc.  shall be run.

  What are the relevant physical parameters.
  What solution algorithm within Nekton to use, what timestep size
  or Courant number to use, or whether to run variable DT, etc.


Section II:   Mesh Geometry Info

  This section gives the number of elements, followed by the
  4 (8) vertex pairs (triplets) which specify the corner of
  each two- (three-) dimensional element.

  A subsection which follows specifies which element surfaces
  a curved.  The exact parameter definitions vary according to
  the type of curved surface.  I usually make my own definitions 
  when I need to generate a new curved surface, e.g., spheres
  or ellipsoids.  We use the Gordon-Hall mapping to generate
  the point distribution within elements, and map according to
  arc-length for points along the edges of two-dimensional 
  elements.  For 3D spheres, I take the element surface point
  distribution to be the intersection of the great circles
  generated by the corresponding edge vertices, and again use
  Gordon-Hall to fill the element interior volume.

  The next subsection contains all the boundary condition information
  for each element.  "E" means that the element is connected to 
  the element specifed by parameter 1, on the side specified by
  parameter 2.  "T" means a constant temperature along the edge
  of the element, "t" means a temperature distribution according
  to a fortran function which the user writes and links in with
  source code contained in "subuser.f".   Similarly, "V" means
  a constant velocity along the element edge (three components
  specifed in parameters 1, 2, and 3, resp.), and "v" implies a
  user specifed fortran function.  "O" is for outflow, "SYM" is
  for symmetry bc's., etc.

Section III:   Output Info.

  This section specifies what data should be output, including
  field dumps (i.e., X,Y,Z + U,V,W + P + T, or any combination
  thereof) and time history trace info... e.g., u(t), v(t), etc.
  at a particular point, for each time step.  (Corresponds to a
  hot wire probe output in some sense.)

  Also, if this run is to restart from a previous field dump (e.g.,
  one computed with a lower order, but the same spectral element
  configuration), that is specified in this section.

This is very brief, but it should give you a road map to the 
general layout of the input files.

------------------------------------------------------------------------

Using PRENEK

This is a brief description of how to modify an existing mesh.
Typically it's easiest to modify run parameters just by editing
the file.  If you wish to modify the geometry, it's (generally)
best to do this through prenek.  To do this,  type "pre"  
(to begin executing prenek).

Then, click on "READ PREVIOUS PARAMETERS" 

Enter "box"  with the keyboard.

Then "ACCEPT PARAMETERS"

Then, "BUILD FROM FILE"

Just hit <cr> to accept the default, box.rea.

Then,  ACCEPT...  ACCEPT... ACCEPT... EXIT

Type a "1" on the keyboard to get a formatted .rea file.

-------------------------------------------------------------------------

Ok - hopefully you now have a good .rea file.

You can edit it to change the 

    viscosity
    number of steps
    Courant number (typ.=2)
    Torder (order of time stepping - typ. 1 in the beginning, 
            for more stability, then Torder=2 when you are after
            sensitive results...)
    IOSTEP  (frequency of .fld dumps)
    etc.

I can help you with these settings.

==============================================================================
STARTING THE JOB: 

   If you've compiled your code (with LELT=LELV=sufficiently large), you
   should now be set to run nekton.  I'll give you some shell scripts,
   (I keep these in my /bin directory...)

   nek:    runs interactively, 

   nekl:   runs interactively, but redirects std. out to job.log ( & logfile)

   nekb:   like nekl, but in the background

==============================================================================
ENDING THE JOB: 

   The job will quit after either FINTIME is reached (I never use this)
   or NSTEPS have completed.

   Should you wish to terminate sooner, or get an output *right now*, I've
   implemented a little check for a file called "ioinfo" into the code.

      To get an output at the end of the current step, but keep on running,
      type:  "echo 1 > ioinfo".

      To get an output at the end of the current step, and then stop the job,
      type:  "echo -1 > ioinfo".

      To stop the job without dumping the .fld file,
      type:  "echo -2 > ioinfo".
==============================================================================

CHANGING BOUNDARY CONDITIONS, INITIAL CONDITIONS, ETC.

I've set the genbox2.dat and genbox2.small files to specify fortran boundary 
conditions at inflow (denoted by lower-case characters)

These are computed in the user accessible source code 
in "../src/subuser.f"  (SUBROUTINE USERBC and SUBROUTINE USERIC).

In the subuser.f file which I gave you,  I specified
a blasius profile for the initial and boundary conditions.
This is the same code I used for my hemisphere runs 

                          ***NOTE****  
You probably will want to change the boundary layer thickness (delta) in 
subuser.f.  Just edit subuser.f, search for where delta is specified, change
it, and go back to 3d6 (3d8? 3d10?) and type "makenek".   We can very easily
make this a runtime parameter, if you find that you're making many trials
with different boundary layer thicknesses.

=============================================================================

RESTARTING

This is really easy...

Just take the .rea file you're about to start with, go to the bottom
of the file, go up ~33 lines, and change the line:

"            0 PRESOLVE/RESTART OPTIONS  *****"

to:
"            1 PRESOLVE/RESTART OPTIONS  *****"
"my_old_run"

or:
"            1 PRESOLVE/RESTART OPTIONS  *****"
"my_old_run.fld"

or:
"            1 PRESOLVE/RESTART OPTIONS  *****"
"my_old_run.fld01  TIME=0.0 "

etc....  (note, drop quotations)

Note that the new run must have the same topology as the old run,
that is, the same number of elements in the same location.  However,
you can change polynomial degree by running in a different directory
(e.g., 3d8) with a code which has been compiled with a different
SIZEu file.

--------------------------------------------------------------------------

Files needed to start a particular run called, say, "hemi1":

    hemi1.rea
    hemi1.sep
    hemi1.map

If hemi1 is to use an old field file (say, "hemi_old.fld23") as an 
initial condition, you will also need to have:

    hemi_old.fld23

If hemi_old.fld23 is in binary format, you will need the associated
header file (which is ascii):

    hemi_old.fhd23

Note that hemi_old must have the same number of elements as hemi1.
However, it can be of different polynomial degree, provided it is
of degree less than or equal to NX1+2,  where NX1 is the number
of grid points in each direction within each element.
(Note:  NX1 := polynomial degree + 1.)


In addition to the above hemi1-specific files, you will also need
a job specific "subuser.f" file to be compiled and linked in with
the source code.   Frequently this file is the same over a broad
range of parametric studies.   It incorporates the user defined
forcing functions, boundary conditions, and initial conditions (if
not restarting from a previous .fld file).   Hence, it's likely
that you will not need to edit/recompile this routine very often.

Note that if you are simultaneously undertaking two *different*
Nekton studies in two different directories which both point to
the same source, you will need to swap subuser.f files each time
you recompile the source in either directory.   I usually do this
by keeping the subuser.f files in the src directory under the names,
e.g.,  hemi.user, cylinder.user, channel.user, etc.   Then when I'm
compiling the source code for the hemisphere problem I would go to
the /src directory and copy hemi.user to subuser.f prior to compiling
in the working directory.


-----------------------------------------------------------------------------

To build nekton,

cd 3d6
makenek

-----------------------------------------------------------------------------
To run nekton in foreground

../bin/nek hemi1

To run nekton in background

../bin/nekb hemi1

-----------------------------------------------------------------------------

To terminate a job (other than "kill")

echo  1 > ioinfo        --- dumps fld file after current step and continues

echo -1 > ioinfo        --- dumps fld file at end of step and quits

echo -2 > ioinfo        --- quits at end of step. No fld file dumped.

echo -# > ioinfo  (#>2) --- quits at end of step #, after dumping fld file.

-----------------------------------------------------------------------------

To run with multiple passive scalars (e.g., temperature and one or more other
convected fields), you should do the following.

0)  Make sure that you have a version of nek5000 and postx that works with 
    multiple passive scalars.  (If you receive these after 9/9/01, you 
    probably do.)

1)  Assuming you already have a .rea file set for Navier-Stokes plus heat 
    transfer, you can use prex to generate a new .rea/.map/.sep file set 
    that allows for an additional passive scalar.   Simply start prenek in 
    the director of interest.  Read parameters from your existing .rea file, 
    then select

         ALTER PARAMETERS

         PASSIVE SCALAR    (1)

         WITH CONVECTION   (Y)

    Hit <cr> through all the parameters (edit these in the .rea file later, 
    with the editor of your choice).   Then, read the geometry from your 
    existing file.  When you get to the boundary condition menu, you will be 
    prompted for BC's for the new passive scalar.   SET ENTIRE LEVEL,  
    Insulated, is a common choice.

2)  Exit prenek.    Note that the default conductivity and rhocp for the 
    new passive scalar is (1.0,1.0).   These can be changed by editing the 
    .rea file (if you're not making them functions of time or space).  
    Simply locate the following lines (found right after the parameters)


      4  Lines of passive scalar data follows2 CONDUCT; 2RHOCP
   0.00100       1.00000       1.00000       1.00000       1.00000
   1.00000       1.00000       1.00000       1.00000
   2.00000       1.00000       1.00000       1.00000       1.00000
   1.00000       1.00000       1.00000       1.00000


    The first 2 lines are the conductivities for the 9 passive scalars.
    You only need to set the first of these.  In the example above,
    we have conduct(PS1) =.001.

    The second 2 lines are the rhocp values for the 9 passive scalars.
    You only need to set the first of these.  In the example above,
    we have rhocp(PS1) =2.0.

/homes/fischer/f/nek5/src
Thu Sep 27 15:56:29 CDT 2001
/nfs/proj-flash/flash/fischer/nek5/src
Fri Oct 26 16:35:44 CDT 2001
/nfs/proj-flash/flash/fischer/nek5/src
Wed Dec 19 13:23:25 CST 2001
/homes/fischer/f/nek5/src
Wed Jan  2 10:41:12 CST 2002
/homes/fischer/f/nek5/src
Wed Jan  2 10:42:01 CST 2002
/nfs/mcs-homes16/fischer/tools/src
Tue Jan  8 10:58:54 CST 2002
/nfs/mcs-homes16/fischer/tools/src
Sat Jan 12 07:39:56 CST 2002
/nfs/mcs-homes16/fischer/tools/src
Sat Jan 19 05:22:35 CST 2002
/nfs/mcs-homes16/fischer/tools/src
Sat Jan 19 05:22:49 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Jan 20 14:15:03 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Jan 20 17:14:47 CST 2002
/homes/fischer/f/nek5/src
Tue Jan 22 11:24:12 CST 2002
/homes/fischer/f/nek5/src
Sun Jan 27 16:10:30 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Jan 27 21:37:16 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Jan 27 21:40:00 CST 2002
/homes/fischer/f/nek5/src
Fri Feb  8 16:28:43 CST 2002
/homes/fischer/f/nek5/src
Fri Feb  8 17:25:37 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Feb 11 14:13:06 CST 2002
/homes/fischer/f/nek5/src
Wed Feb 13 15:36:39 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sat Feb 23 14:19:20 CST 2002
/homes/fischer/f/nek5/src
Sat Feb 23 16:33:09 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Feb 24 16:51:44 CST 2002
/homes/fischer/f/nek5/src
Tue Feb 26 13:37:30 CST 2002
/homes/fischer/f/nek5/src
Thu Mar 14 09:18:40 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Mar 18 17:41:46 CST 2002
/homes/fischer/f/nek5/src
Wed May  1 12:07:53 CDT 2002
/homes/fischer/f/nek5/src
Mon May  6 10:33:16 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 22 15:00:14 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 22 23:10:06 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 22 23:17:36 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 22 23:20:26 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 22 23:21:05 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 29 10:33:02 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Wed May 29 22:36:28 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Thu May 30 09:15:57 CDT 2002
/homes/fischer/f/nek5/src
Thu May 30 09:53:28 CDT 2002
/homes/fischer/f/nek5/src
Wed Jun 19 06:02:05 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Jul  7 20:41:15 CDT 2002
/homes/fischer/f/nek5/src
Tue Jul  9 19:29:17 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Aug 12 09:33:59 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Aug 12 09:59:34 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Aug 12 10:18:15 CDT 2002
/homes/fischer/f/nek5/src
Mon Aug 12 11:05:06 CDT 2002
/homes/fischer/f/nek5/src
Mon Aug 12 14:20:27 CDT 2002
/homes/fischer/f/nek5/src
Mon Aug 12 14:22:09 CDT 2002
/homes/fischer/f/nek5/src
Wed Aug 14 15:27:52 CDT 2002
/homes/fischer/f/nek5/src
Tue Sep  3 16:21:23 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Sun Sep 29 13:45:23 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Sep 30 04:19:13 CDT 2002
/nfs/proj-flash/flash/fischer/nek5/src
Mon Sep 30 14:17:03 CDT 2002
/homes/fischer/f/nek5/src
Tue Oct  8 04:37:18 CDT 2002
/homes/fischer/f/nek5/src
Fri Nov 15 10:52:09 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Tue Nov 19 21:03:45 CST 2002
/nfs/proj-flash/flash/fischer/nek5/src
Thu Nov 21 10:47:24 CST 2002
/homes/fischer/f/nek5/src
Tue Dec 10 13:59:39 CST 2002
/homes/fischer/f/nek5/src
Mon Jan  6 15:59:07 CST 2003
/homes/fischer/f/nek5/src
Mon Jan 20 11:24:22 CST 2003
/homes/fischer/f/nek5/src
Fri Apr 11 10:10:44 CDT 2003
/homes/fischer/f/nek5/src
Thu Jun  5 10:08:20 CDT 2003
/homes/fischer/b3/james/src_mg
Mon Jan 26 15:15:28 CST 2004
/homes/fischer/b3/james/src_mg
Mon Jan 26 20:47:06 CST 2004
/homes/fischer/b3/james/src_mg
Mon Jan 26 20:53:20 CST 2004
/homes/fischer/b3/james/src_mg
Tue Jan 27 08:40:36 CST 2004
/home/fischer/james/src_mg
Tue Jan 27 14:42:02 CST 2004
/home/fischer/james/src_mg
Fri Jan 30 19:25:01 CST 2004
/home/fischer/james/src
Tue Mar 23 17:15:11 CST 2004
/home/fischer/james/src
Thu Apr 29 07:23:32 CDT 2004
/home/fischer/james/src
Tue Jun 29 17:09:50 CDT 2004
/homes/fischer/f/nek5dg/src
Fri Mar  4 13:35:51 CST 2005
/homes/mmin/NEK5DG/nek5dg/Misun/src_std
Fri Apr 22 16:55:03 CDT 2005
/homes/mmin/NEK5DG/nek5dg/Misun/src_std
Fri Apr 22 17:57:09 CDT 2005
/homes/mmin/NEK5DG/nek5dg/Misun3D/src_std
Fri Sep  9 16:00:13 CDT 2005
/home/fischer/nek5dg/src_stdc
Mon Sep 26 23:38:44 CDT 2005
/homes/fischer/s/nek5dg/src_stdc3
Mon Oct  3 15:57:52 CDT 2005
/homes/fischer/s/nek5dg/src_stdc
Mon Oct  3 16:55:55 CDT 2005
/sandbox/mmin/smash/src_stdc
Wed Oct  5 16:37:21 CDT 2005
/sandbox/mmin/smash/src_stdc
Wed Oct  5 17:55:21 CDT 2005
/sandbox/mmin/smash/src_stdc
Wed Oct  5 17:55:31 CDT 2005
/sandbox/mmin/smash/src_stdc2
Wed Oct  5 18:14:29 CDT 2005
/sandbox/mmin/smash/src_stdc2
Wed Oct  5 18:14:53 CDT 2005
/home/fischer/cem/src_stdc
Thu Oct  6 14:26:02 CDT 2005
/homes/mmin/waveguide_clean/src_stdc_d
Wed Apr 12 17:08:06 CDT 2006
/homes/mmin/nekc/src
Sun May 28 23:20:50 CDT 2006
/homes/mmin/nekc/src
Sun May 28 23:24:08 CDT 2006
